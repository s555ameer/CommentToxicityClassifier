{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b43a962-f1cb-4cf8-94d7-dea17e8531a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./automl/lib/python3.9/site-packages (2.11.0)\n",
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow_gpu-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:05\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Using cached sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in ./automl/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./automl/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./automl/lib/python3.9/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: numpy>=1.20 in ./automl/lib/python3.9/site-packages (from tensorflow) (1.24.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in ./automl/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./automl/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./automl/lib/python3.9/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./automl/lib/python3.9/site-packages (from tensorflow) (0.29.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in ./automl/lib/python3.9/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in ./automl/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./automl/lib/python3.9/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: setuptools in ./automl/lib/python3.9/site-packages (from tensorflow) (58.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./automl/lib/python3.9/site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./automl/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: packaging in ./automl/lib/python3.9/site-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in ./automl/lib/python3.9/site-packages (from tensorflow) (2.11.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./automl/lib/python3.9/site-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in ./automl/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./automl/lib/python3.9/site-packages (from tensorflow) (23.1.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./automl/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: six>=1.12.0 in ./automl/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./automl/lib/python3.9/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./automl/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.7/299.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./automl/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./automl/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./automl/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./automl/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./automl/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./automl/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./automl/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./automl/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./automl/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./automl/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./automl/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./automl/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./automl/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./automl/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./automl/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./automl/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./automl/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./automl/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./automl/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./automl/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./automl/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=121098c3a88c815a4798370b750ab3e90dc0c94fe0078fdbdcc501e290aca24c\n",
      "  Stored in directory: /home/sourav/.cache/pip/wheels/03/8b/6f/9f13c705de81a6b351b718b3cf917e41ad7c0933c8630d4dd4\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, tensorflow-gpu\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.3 pandas-1.5.3 pillow-9.4.0 pyparsing-3.0.9 sklearn-0.0.post1 tensorflow-gpu-2.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow tensorflow-gpu pandas matplotlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2bf6872-7d82-483d-85b4-f1345bc6d2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 01:36:59.686237: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-22 01:37:00.250223: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-22 01:37:00.250253: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-22 01:37:01.750499: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 01:37:01.750618: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 01:37:01.750630: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c8a7e7-7df2-44ff-aad6-5bc1b5b94728",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jigsaw-toxic-comment-classification-challenge/train.csv/train.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('jigsaw-toxic-comment-classification-challenge','train.csv','train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423de888-904b-4011-bf16-7e3ea76ecdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('jigsaw-toxic-comment-classification-challenge','train.csv','train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386d6111-fd2e-4630-8b25-abd90d20e282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f986e6e-bbf8-4782-acdb-f46a4553d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdab487-0950-4d86-a835-8be08e9a2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be133a66-40c0-4355-ae99-f08340647180",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =df['comment_text']\n",
    "y = df[df.columns[2:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fe1ffd-f3f3-4eab-ba77-9daab08dddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 20000 #number of words in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc49635-5ee0-4abb-8423-d50d9ff1cd7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 01:37:04.524877: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-22 01:37:04.525378: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-22 01:37:04.525447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sourav-HP-Laptop-15-da0xxx): /proc/driver/nvidia/version does not exist\n",
      "2023-01-22 01:37:04.526931: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TextVectorization(max_tokens = MAX_FEATURES,\n",
    "                              output_sequence_length = 1800,\n",
    "                              output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8ef3b9-98b3-4c67-a31a-e607da25312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93bfc617-dfc6-41f9-9bf3-daf0e48c158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 01:37:18.840882: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2297822400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "vectorized_text = vectorizer(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f00982fe-47bd-4656-9853-0fca750bff61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(159571, 1800), dtype=int64, numpy=\n",
       "array([[ 645,   76,    2, ...,    0,    0,    0],\n",
       "       [   1,   54, 2489, ...,    0,    0,    0],\n",
       "       [ 425,  441,   70, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1, 7392,  383, ...,    0,    0,    0],\n",
       "       [   5,   12,  534, ...,    0,    0,    0],\n",
       "       [   5,    8,  130, ...,    0,    0,    0]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0cfef0f-4ae5-47c9-931b-e6729c26fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,y))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(16000)\n",
    "dataset = dataset.batch(16)\n",
    "dataset = dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b796c1cc-9b83-4dda-9404-3b1edfe50649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 01:37:20.579287: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2297822400 exceeds 10% of free system memory.\n",
      "2023-01-22 01:37:22.791123: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "batch_x,batch_y = dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fd0e0b0-b065-40ae-9a8a-fc9cb58b1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a14ceb8-5420-4d58-82e8-e224ca8776dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9974"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a387562-5478-49cb-8063-a32580fdafae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.take(int(len(dataset)*.7))\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7fddb92-d69d-49bc-85b9-c721a06803cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 01:37:22.889996: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2297822400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbe16ee7-8a77-4ea1-b296-5af87024cb99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1263, 1051,  468, ...,    0,    0,    0],\n",
       "        [ 218,   17,  157, ...,    0,    0,    0],\n",
       "        [ 485,   58, 1652, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 225,  568,  654, ...,    0,    0,    0],\n",
       "        [ 109,    4,  407, ...,    0,    0,    0],\n",
       "        [   6,  621,   32, ...,    0,    0,    0]]),\n",
       " array([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "603cda04-0411-46f9-a362-02bd259c6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37dbd659-2826-4204-8932-5354954f0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d63a70b-6c38-430c-b717-fcd02d3a3c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_FEATURES+1, 32))\n",
    "model.add(Bidirectional(LSTM(32,activation='tanh')))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07f0dedb-dcd6-4cb6-b192-e2738adc3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89004c31-b814-4238-ae9a-9465be8f146b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          640032    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               16640     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 731,686\n",
      "Trainable params: 731,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f5908-9ca5-417c-8113-f177431cd0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 891/6981 [==>...........................] - ETA: 1:28:20 - loss: 0.1120"
     ]
    }
   ],
   "source": [
    "history = model.fit(train, epochs=1, validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e02c96-43ad-4644-ad14-be3eb9c8fd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c00362-f9ea-4c99-99cd-c501a7b65f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce0b03-5b7b-4e91-a341-e937b0bd98a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648a20e-d7f2-49cc-8b76-3fef8a2d2449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c508d3-944a-4b7c-a6bf-c84ed036f7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl",
   "language": "python",
   "name": "automl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
